# –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ç–æ—Ç–∏–ø –ò–ò-—Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—é –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–ª–æ–≤
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É torchaudio –∏ pre-trained –º–æ–¥–µ–ª—å wav2vec 2.0 + Web-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ Streamlit

import torchaudio
import torchaudio.transforms as T
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import soundfile as sf
import os
from glob import glob
import pronouncing
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import csv
import shutil
import streamlit as st
from io import BytesIO
import base64
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase
import av

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ ---
# –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º –º–æ–¥–µ–ª–∏
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã
local_model_path = "./wav2vec2-local"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
try:
    processor = Wav2Vec2Processor.from_pretrained(local_model_path)
    model = Wav2Vec2ForCTC.from_pretrained(local_model_path)
except OSError as e:
    st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ '{local_model_path}': {e}\n"
             f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—É—Ç—å —É–∫–∞–∑–∞–Ω –≤–µ—Ä–Ω–æ –∏ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ —Å–∫–∞—á–∞–Ω—ã.")
    st.stop() # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

model.eval()

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Common Voice (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
def extract_common_voice(target_words, source_dir, out_dir, max_per_word=10):
    tsv_file = os.path.join(source_dir, "validated.tsv")
    clips_dir = os.path.join(source_dir, "clips")
    os.makedirs(out_dir, exist_ok=True)
    counters = {w: 0 for w in target_words}

    with open(tsv_file, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for row in reader:
            sentence = row["sentence"].lower().strip()
            for word in target_words:
                if word in sentence.split() and counters[word] < max_per_word:
                    src_audio = os.path.join(clips_dir, row["path"])
                    dst_dir = os.path.join(out_dir, word)
                    os.makedirs(dst_dir, exist_ok=True)
                    dst_audio = os.path.join(dst_dir, row["path"])
                    shutil.copy(src_audio, dst_audio)
                    counters[word] += 1
                    break
    print("‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", out_dir)

# –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ –∏–ª–∏ —Ç–µ–Ω–∑–æ—Ä–∞
def transcribe(audio_path=None, waveform=None, sampling_rate=None):
    if audio_path:
        speech_array, sampling_rate = torchaudio.load(audio_path)
    elif waveform is not None and sampling_rate is not None:
        speech_array = waveform
    else:
        raise ValueError("Either audio_path or waveform and sampling_rate must be provided")

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —É –Ω–∞—Å 1 –∫–∞–Ω–∞–ª (–º–æ–Ω–æ)
    if speech_array.ndim > 1 and speech_array.shape[0] > 1:
        speech_array = torch.mean(speech_array, dim=0, keepdim=True)

    resampler = T.Resample(orig_freq=sampling_rate, new_freq=16000)
    speech = resampler(speech_array).squeeze()
    inputs = processor(speech, sampling_rate=16000, return_tensors="pt", padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.decode(predicted_ids[0])
    return transcription.lower()

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–æ–Ω–µ–º –∏–∑ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ CMU Pronouncing Dictionary
def get_phonemes(word):
    phones = pronouncing.phones_for_word(word)
    return phones[0] if phones else None

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç—Ç–∞–ª–æ–Ω–æ–º –∏ —Ñ–æ–Ω–µ–º–∞–º–∏ + –æ—Ç—á—ë—Ç
def compare_with_target(predicted, target):
    result = {}
    result["prediction"] = predicted
    result["expected"] = target
    result["match"] = predicted == target.lower()
    result["expected_phones"] = get_phonemes(target)
    result["predicted_phones"] = get_phonemes(predicted)
    return result

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è waveform –∏ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã
def visualize_audio(audio_path=None, y=None, sr=None):
    if audio_path:
        y, sr = librosa.load(audio_path, sr=16000) # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ä–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –¥–æ 16–∫–ì—Ü
    elif y is None or sr is None:
         raise ValueError("Either audio_path or y and sr must be provided")
    elif sr != 16000:
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ, –Ω–æ –Ω–µ —Å —Ç–æ–π —á–∞—Å—Ç–æ—Ç–æ–π –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, —Ä–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º
        y = librosa.resample(y, orig_sr=sr, target_sr=16000)
        sr = 16000

    fig, axs = plt.subplots(2, 1, figsize=(10, 4))
    librosa.display.waveshow(y, sr=sr, ax=axs[0])
    axs[0].set_title("Waveform")
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º librosa.stft –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ
    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axs[1])
    axs[1].set_title("Spectrogram")
    fig.colorbar(img, ax=axs[1]) # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ colorbar –ø—Ä–∏–≤—è–∑–∞–Ω –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ—Å–∏
    plt.tight_layout() # –î–æ–±–∞–≤–∏–º –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è
    return fig

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–∞—Ö
def generate_report(result):
    report_lines = []
    report_lines.append(f"–û–∂–∏–¥–∞–ª–æ—Å—å —Å–ª–æ–≤–æ: {result['expected']}")
    report_lines.append(f"–ü—Ä–æ–∏–∑–Ω–µ—Å–µ–Ω–æ: {result['prediction']}")
    report_lines.append(f"–§–æ–Ω–µ–º—ã (—ç—Ç–∞–ª–æ–Ω): {result['expected_phones']}")
    report_lines.append(f"–§–æ–Ω–µ–º—ã (–≤–∞—à–∏): {result['predicted_phones']}")
    match_str = "‚úÖ –°–æ–≤–ø–∞–¥–∞–µ—Ç —Å —ç—Ç–∞–ª–æ–Ω–æ–º" if result['match'] else "‚ùå –ï—Å—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è"
    report_lines.append(match_str)
    return "\n".join(report_lines)

# –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç –∫–∞–∫ .txt —Ñ–∞–π–ª
def get_download_link(text, filename="feedback.txt"):
    b64 = base64.b64encode(text.encode()).decode()
    href = f'<a href="data:file/txt;base64,{b64}" download="{filename}">üìÑ –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç</a>'
    return href

# Streamlit-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("üéôÔ∏è –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–ª–æ–≤")
target_word = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–æ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:", "photograph")
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ .wav —Ñ–∞–π–ª:", type=["wav"])

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
if uploaded_file is not None:
    # –ß–∏—Ç–∞–µ–º –±–∞–π—Ç—ã —Ñ–∞–π–ª–∞
    audio_bytes = uploaded_file.getvalue()
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ç–µ–Ω–∑–æ—Ä –∏–∑ –±–∞–π—Ç–æ–≤
    waveform, sr = torchaudio.load(BytesIO(audio_bytes))

    # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –∏ —á–∞—Å—Ç–æ—Ç—É –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –≤ transcribe
    predicted = transcribe(waveform=waveform, sampling_rate=sr)
    result = compare_with_target(predicted, target_word)
    report = generate_report(result)

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∞—É–¥–∏–æ –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –±–∞–π—Ç–æ–≤
    st.audio(audio_bytes, format="audio/wav")
    st.write(report)
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ numpy)
    st.pyplot(visualize_audio(y=waveform.numpy().squeeze(), sr=sr))
    st.markdown(get_download_link(report), unsafe_allow_html=True)

# –í–∫–ª—é—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (streamlit-webrtc)
class AudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.frames = []

    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        self.frames.append(frame)
        return frame

from streamlit_webrtc import WebRtcMode

ctx = webrtc_streamer(
    key='mic_stream_',
    mode=WebRtcMode.SENDRECV,
    audio_receiver_size=256,
    media_stream_constraints={"audio": True, "video": False},
    audio_processor_factory=AudioProcessor,
    async_processing=True,
)

# –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏
if ctx.audio_processor:
    if not ctx.state.playing and len(ctx.audio_processor.frames) > 0:
        st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏...")
        frames = ctx.audio_processor.frames
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ—Ä–µ–π–º—ã –≤ –æ–¥–∏–Ω numpy –º–∞—Å—Å–∏–≤
        samples = np.concatenate([f.to_ndarray()[0] for f in frames]).astype(np.float32)
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –º–∞—Å—Å–∏–≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π (–º–æ–Ω–æ)
        if samples.ndim > 1:
            samples = np.mean(samples, axis=1) # –ò–ª–∏ samples = samples[:, 0] –µ—Å–ª–∏ –∑–Ω–∞–µ–º, —á—Ç–æ –∑–≤—É–∫ –≤ –ø–µ—Ä–≤–æ–º –∫–∞–Ω–∞–ª–µ

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy –≤ torch —Ç–µ–Ω–∑–æ—Ä –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ –¥–ª—è –±–∞—Ç—á–∞/–∫–∞–Ω–∞–ª–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ torchaudio
        waveform = torch.tensor(samples).unsqueeze(0)
        sr = 16000 # –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ WebRTC –æ–±—ã—á–Ω–æ 16000 –∏–ª–∏ 48000, –Ω–æ Wav2Vec —Ç—Ä–µ–±—É–µ—Ç 16000

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –∏–∑ —Ç–µ–Ω–∑–æ—Ä–∞
        predicted = transcribe(waveform=waveform, sampling_rate=sr)
        result = compare_with_target(predicted, target_word)
        report = generate_report(result)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤ –±–∞–π—Ç–æ–≤—ã–π –±—É—Ñ–µ—Ä –¥–ª—è st.audio
        buffer = BytesIO()
        torchaudio.save(buffer, waveform, sr, format="wav")
        buffer.seek(0)

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        st.audio(buffer, format="audio/wav")
        st.write(report)
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑ numpy –º–∞—Å—Å–∏–≤–∞
        st.pyplot(visualize_audio(y=samples, sr=sr))
        st.markdown(get_download_link(report), unsafe_allow_html=True)

        # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä —Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–ø–∏—Å–∏
        ctx.audio_processor.frames = []
        st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

elif ctx.state.playing:
    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–ø–∏—Å–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    st.write("–ò–¥–µ—Ç –∑–∞–ø–∏—Å—å... –ù–∞–∂–º–∏—Ç–µ 'Stop' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")

# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –±–ª–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ –≤—Ä–µ–º—è –∑–∞–ø–∏—Å–∏
# if ctx and ctx.state.playing and ctx.audio_processor and len(ctx.audio_processor.frames) > 0:
#     frames = ctx.audio_processor.frames
#     samples = np.concatenate([f.to_ndarray()[0] for f in frames]).astype(np.float32)
#     audio_path = "mic_recording.wav"
#     # –ù—É–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ñ–æ—Ä–º–∞—Ç –∏ —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º –º–æ–¥–µ–ª–∏
#     # Wav2Vec –æ–∂–∏–¥–∞–µ—Ç 16000 –ì—Ü, –º–æ–Ω–æ
#     # av —Ñ—Ä–µ–π–º—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å –¥—Ä—É–≥—É—é —á–∞—Å—Ç–æ—Ç—É, –ø—Ä–æ–≤–µ—Ä–∏–º –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º
#     sample_rate_mic = frames[0].sample_rate
#     waveform_mic = torch.tensor([samples])
#     if sample_rate_mic != 16000:
#         resampler_mic = T.Resample(orig_freq=sample_rate_mic, new_freq=16000)
#         waveform_mic = resampler_mic(waveform_mic)
#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ
#     torchaudio.save(audio_path, waveform_mic, 16000)
#
#     predicted = transcribe(audio_path=audio_path)
#     result = compare_with_target(predicted, target_word)
#     report = generate_report(result)
#
#     st.audio(audio_path, format="audio/wav")
#     st.write(report)
#     st.pyplot(visualize_audio(audio_path=audio_path))
#     st.markdown(get_download_link(report), unsafe_allow_html=True)
